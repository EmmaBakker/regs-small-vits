{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "from timm.models import create_model\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, Normalize, ToTensor\n",
    "from model.deit_reg.models_v2 import deit_small_patch16_LS, Mlp\n",
    "from model.deit_reg import models_v2\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import Imagenette\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title get image paths\n",
    "def get_paths_in_subfolders(folder_path: str) -> list[list[str]]:\n",
    "  \"\"\"Given a folder path, returns a list of all image paths in the subfolders.\n",
    "\n",
    "  Args:\n",
    "    folder_path: The path to the folder.\n",
    "\n",
    "  Returns:\n",
    "    A nested list of all image paths in the subfolders.\n",
    "  \"\"\"\n",
    "\n",
    "  files : list[list[str]] = []\n",
    "  # Iterate through the subfolders in the given folder\n",
    "  for subfolder in os.listdir(folder_path):\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    folder : list[str] = []\n",
    "    if os.path.isdir(subfolder_path):\n",
    "      for file_ in os.listdir(subfolder_path):\n",
    "        file_path = os.path.join(subfolder_path, file_)\n",
    "        folder.append(file_path)\n",
    "\n",
    "    files.append(folder)\n",
    "\n",
    "  return files\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x: Image.Image | list[Image.Image] | torch.Tensor,\n",
    "               size: tuple[int, int] | int = (224, 224)):\n",
    "\n",
    "  if isinstance(size, int):\n",
    "    if not isinstance(x, Image.Image):\n",
    "      raise ValueError(\"size must be a tuple for sequence of images\")\n",
    "\n",
    "    width, height = x.size\n",
    "    size = ((width // size) * size, (height // size) * size)\n",
    "\n",
    "\n",
    "  x = transforms.Resize(size)(x)\n",
    "  x = transforms.ToTensor()(x)\n",
    "  x = transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN,\n",
    "                           std=IMAGENET_DEFAULT_STD)(x)\n",
    "\n",
    "  assert isinstance(x, torch.Tensor)\n",
    "  if len(x.shape) == 3:\n",
    "    x = x.unsqueeze(0)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(img):\n",
    "    transform_fn = Compose([Resize(249, 3), CenterCrop(224), ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    return transform_fn(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f5/0wsbjcm56b5czjl3m5v9l1tm0000gn/T/ipykernel_4705/1391257013.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint =torch.load(f'checkpoints/source_checkpoints/checkpoint799.pth', map_location=torch.device(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 799\n",
      "odict_keys(['cls_token', 'pos_embed', 'patch_embed.proj.weight', 'patch_embed.proj.bias', 'blocks.0.gamma_1', 'blocks.0.gamma_2', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.attn.qkv.weight', 'blocks.0.attn.qkv.bias', 'blocks.0.attn.proj.weight', 'blocks.0.attn.proj.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.fc1.weight', 'blocks.0.mlp.fc1.bias', 'blocks.0.mlp.fc2.weight', 'blocks.0.mlp.fc2.bias', 'blocks.1.gamma_1', 'blocks.1.gamma_2', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.attn.qkv.weight', 'blocks.1.attn.qkv.bias', 'blocks.1.attn.proj.weight', 'blocks.1.attn.proj.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.fc1.weight', 'blocks.1.mlp.fc1.bias', 'blocks.1.mlp.fc2.weight', 'blocks.1.mlp.fc2.bias', 'blocks.2.gamma_1', 'blocks.2.gamma_2', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.attn.qkv.weight', 'blocks.2.attn.qkv.bias', 'blocks.2.attn.proj.weight', 'blocks.2.attn.proj.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.2.mlp.fc1.weight', 'blocks.2.mlp.fc1.bias', 'blocks.2.mlp.fc2.weight', 'blocks.2.mlp.fc2.bias', 'blocks.3.gamma_1', 'blocks.3.gamma_2', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.attn.qkv.weight', 'blocks.3.attn.qkv.bias', 'blocks.3.attn.proj.weight', 'blocks.3.attn.proj.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.3.mlp.fc1.weight', 'blocks.3.mlp.fc1.bias', 'blocks.3.mlp.fc2.weight', 'blocks.3.mlp.fc2.bias', 'blocks.4.gamma_1', 'blocks.4.gamma_2', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.attn.qkv.weight', 'blocks.4.attn.qkv.bias', 'blocks.4.attn.proj.weight', 'blocks.4.attn.proj.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.4.mlp.fc1.weight', 'blocks.4.mlp.fc1.bias', 'blocks.4.mlp.fc2.weight', 'blocks.4.mlp.fc2.bias', 'blocks.5.gamma_1', 'blocks.5.gamma_2', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.attn.qkv.weight', 'blocks.5.attn.qkv.bias', 'blocks.5.attn.proj.weight', 'blocks.5.attn.proj.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.5.mlp.fc1.weight', 'blocks.5.mlp.fc1.bias', 'blocks.5.mlp.fc2.weight', 'blocks.5.mlp.fc2.bias', 'blocks.6.gamma_1', 'blocks.6.gamma_2', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.attn.qkv.weight', 'blocks.6.attn.qkv.bias', 'blocks.6.attn.proj.weight', 'blocks.6.attn.proj.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.6.mlp.fc1.weight', 'blocks.6.mlp.fc1.bias', 'blocks.6.mlp.fc2.weight', 'blocks.6.mlp.fc2.bias', 'blocks.7.gamma_1', 'blocks.7.gamma_2', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.attn.qkv.weight', 'blocks.7.attn.qkv.bias', 'blocks.7.attn.proj.weight', 'blocks.7.attn.proj.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.7.mlp.fc1.weight', 'blocks.7.mlp.fc1.bias', 'blocks.7.mlp.fc2.weight', 'blocks.7.mlp.fc2.bias', 'blocks.8.gamma_1', 'blocks.8.gamma_2', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.attn.qkv.weight', 'blocks.8.attn.qkv.bias', 'blocks.8.attn.proj.weight', 'blocks.8.attn.proj.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.8.mlp.fc1.weight', 'blocks.8.mlp.fc1.bias', 'blocks.8.mlp.fc2.weight', 'blocks.8.mlp.fc2.bias', 'blocks.9.gamma_1', 'blocks.9.gamma_2', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.attn.qkv.weight', 'blocks.9.attn.qkv.bias', 'blocks.9.attn.proj.weight', 'blocks.9.attn.proj.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.9.mlp.fc1.weight', 'blocks.9.mlp.fc1.bias', 'blocks.9.mlp.fc2.weight', 'blocks.9.mlp.fc2.bias', 'blocks.10.gamma_1', 'blocks.10.gamma_2', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.attn.qkv.weight', 'blocks.10.attn.qkv.bias', 'blocks.10.attn.proj.weight', 'blocks.10.attn.proj.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.10.mlp.fc1.weight', 'blocks.10.mlp.fc1.bias', 'blocks.10.mlp.fc2.weight', 'blocks.10.mlp.fc2.bias', 'blocks.11.gamma_1', 'blocks.11.gamma_2', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.attn.qkv.weight', 'blocks.11.attn.qkv.bias', 'blocks.11.attn.proj.weight', 'blocks.11.attn.proj.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.11.mlp.fc1.weight', 'blocks.11.mlp.fc1.bias', 'blocks.11.mlp.fc2.weight', 'blocks.11.mlp.fc2.bias', 'norm.weight', 'norm.bias', 'head.weight', 'head.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vit_models(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Layer_scale_init_Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint =torch.load(f'checkpoints/source_checkpoints/checkpoint799.pth', map_location=torch.device(device))\n",
    "print('epoch', checkpoint['epoch'])\n",
    "\n",
    "model_deit = deit_small_patch16_LS()\n",
    "model_deit.default_cfg = models_v2._cfg()\n",
    "\n",
    "print(checkpoint['model'].keys())\n",
    "\n",
    "model_deit.load_state_dict(checkpoint[\"model\"])\n",
    "model_deit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jipdevries/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocessing steps (resize, crop, convert to tensor, and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "# Create a dataset from the directory structure. \n",
    "# ImageFolder automatically assigns labels based on subfolder names.\n",
    "train_set = datasets.ImageFolder('datasets/imagenette2/train', transform=transform)\n",
    "val_set = datasets.ImageFolder('datasets/imagenette2/val', transform=transform)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create a DataLoader to handle batching, shuffling, and parallel loading.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True, drop_last=True)\n",
    "if torch.cuda.is_available():\n",
    "    num_workers = 16\n",
    "else:\n",
    "    num_workers = 0\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vit_models(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Layer_scale_init_Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_deit.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3\n",
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss', 'val_loss', 'checkpoint_type'])\n",
      "{'epoch': 3, 'model_state_dict': OrderedDict([('fc1.weight', tensor([[ 0.0064,  0.1183,  0.0193,  ..., -0.0141,  0.0582, -0.0209],\n",
      "        [-0.0221,  0.0800,  0.0459,  ...,  0.1286, -0.0131,  0.0527],\n",
      "        [ 0.0017,  0.1089,  0.0709,  ...,  0.0814,  0.0239, -0.0415],\n",
      "        ...,\n",
      "        [ 0.0318,  0.0403,  0.0830,  ...,  0.1119,  0.0247, -0.0090],\n",
      "        [-0.0398,  0.0951,  0.0655,  ..., -0.0409, -0.0029,  0.0260],\n",
      "        [ 0.0350,  0.0948,  0.2028,  ...,  0.0033,  0.0199,  0.0383]])), ('fc1.bias', tensor([-0.1863, -0.2569, -0.3461,  ..., -0.2510, -0.2404, -0.2555])), ('fc2.weight', tensor([[ 0.0086,  0.0087,  0.0046,  ..., -0.0120,  0.0021, -0.0086],\n",
      "        [ 0.0022,  0.0096, -0.0028,  ..., -0.0132, -0.0046, -0.0086],\n",
      "        [ 0.0016,  0.0090, -0.0073,  ..., -0.0124, -0.0052, -0.0108],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0031,  0.0041,  ...,  0.0019, -0.0067, -0.0120],\n",
      "        [-0.0109, -0.0030, -0.0019,  ...,  0.0048, -0.0011, -0.0151],\n",
      "        [-0.0076, -0.0043, -0.0023,  ..., -0.0018, -0.0049, -0.0161]])), ('fc2.bias', tensor([-0.0213, -0.0213, -0.0213, -0.0212, -0.0209, -0.0208, -0.0208, -0.0212,\n",
      "        -0.0211, -0.0209, -0.0209, -0.0210, -0.0209, -0.0208, -0.0205, -0.0200,\n",
      "        -0.0198, -0.0204, -0.0206, -0.0207, -0.0209, -0.0206, -0.0207, -0.0206,\n",
      "        -0.0207, -0.0205, -0.0204, -0.0204, -0.0203, -0.0203, -0.0205, -0.0206,\n",
      "        -0.0207, -0.0207, -0.0202, -0.0201, -0.0201, -0.0201, -0.0200, -0.0202,\n",
      "        -0.0204, -0.0203, -0.0202, -0.0205, -0.0204, -0.0202, -0.0201, -0.0200,\n",
      "        -0.0198, -0.0200, -0.0199, -0.0199, -0.0198, -0.0197, -0.0195, -0.0195,\n",
      "        -0.0194, -0.0194, -0.0195, -0.0192, -0.0193, -0.0192, -0.0193, -0.0193,\n",
      "        -0.0206, -0.0207, -0.0207, -0.0204, -0.0201, -0.0199, -0.0199, -0.0203,\n",
      "        -0.0204, -0.0202, -0.0202, -0.0207, -0.0206, -0.0206, -0.0200, -0.0199,\n",
      "        -0.0199, -0.0198, -0.0200, -0.0203, -0.0202, -0.0200, -0.0199, -0.0200,\n",
      "        -0.0203, -0.0203, -0.0200, -0.0198, -0.0200, -0.0198, -0.0200, -0.0195,\n",
      "        -0.0212, -0.0217, -0.0217, -0.0212, -0.0212, -0.0218, -0.0219, -0.0220,\n",
      "        -0.0220, -0.0220, -0.0220, -0.0219, -0.0218, -0.0213, -0.0210, -0.0210,\n",
      "        -0.0210, -0.0211, -0.0213, -0.0213, -0.0212, -0.0213, -0.0214, -0.0217,\n",
      "        -0.0217, -0.0217, -0.0217, -0.0221, -0.0221, -0.0219, -0.0220, -0.0217,\n",
      "        -0.0216, -0.0214, -0.0213, -0.0211, -0.0214, -0.0215, -0.0219, -0.0222,\n",
      "        -0.0224, -0.0220, -0.0220, -0.0221, -0.0221, -0.0218, -0.0219, -0.0216,\n",
      "        -0.0213, -0.0217, -0.0216, -0.0215, -0.0217, -0.0218, -0.0215, -0.0217,\n",
      "        -0.0218, -0.0220, -0.0216, -0.0218, -0.0216, -0.0213, -0.0211, -0.0208,\n",
      "        -0.0215, -0.0216, -0.0215, -0.0210, -0.0214, -0.0216, -0.0218, -0.0221,\n",
      "        -0.0220, -0.0219, -0.0221, -0.0221, -0.0221, -0.0222, -0.0222, -0.0217,\n",
      "        -0.0216, -0.0219, -0.0222, -0.0221, -0.0221, -0.0219, -0.0216, -0.0216,\n",
      "        -0.0216, -0.0218, -0.0216, -0.0216, -0.0218, -0.0220, -0.0223, -0.0221,\n",
      "        -0.0238, -0.0237, -0.0236, -0.0232, -0.0233, -0.0233, -0.0235, -0.0236,\n",
      "        -0.0233, -0.0233, -0.0235, -0.0235, -0.0235, -0.0234, -0.0232, -0.0231,\n",
      "        -0.0232, -0.0235, -0.0232, -0.0231, -0.0231, -0.0232, -0.0232, -0.0230,\n",
      "        -0.0228, -0.0229, -0.0231, -0.0232, -0.0234, -0.0234, -0.0232, -0.0231,\n",
      "        -0.0230, -0.0228, -0.0227, -0.0225, -0.0225, -0.0224, -0.0223, -0.0224,\n",
      "        -0.0224, -0.0223, -0.0223, -0.0224, -0.0223, -0.0224, -0.0221, -0.0217,\n",
      "        -0.0213, -0.0220, -0.0221, -0.0219, -0.0222, -0.0220, -0.0219, -0.0221,\n",
      "        -0.0220, -0.0220, -0.0220, -0.0221, -0.0220, -0.0221, -0.0222, -0.0222,\n",
      "        -0.0207, -0.0205, -0.0201, -0.0200, -0.0200, -0.0201, -0.0200, -0.0199,\n",
      "        -0.0203, -0.0202, -0.0203, -0.0201, -0.0202, -0.0199, -0.0200, -0.0198,\n",
      "        -0.0196, -0.0199, -0.0197, -0.0195, -0.0193, -0.0192, -0.0189, -0.0189,\n",
      "        -0.0191, -0.0190, -0.0191, -0.0190, -0.0191, -0.0191, -0.0191, -0.0191,\n",
      "        -0.0204, -0.0204, -0.0203, -0.0200, -0.0197, -0.0195, -0.0193, -0.0197,\n",
      "        -0.0198, -0.0197, -0.0199, -0.0204, -0.0200, -0.0200, -0.0198, -0.0196,\n",
      "        -0.0194, -0.0196, -0.0198, -0.0199, -0.0196, -0.0198, -0.0199, -0.0199,\n",
      "        -0.0200, -0.0198, -0.0197, -0.0193, -0.0193, -0.0194, -0.0192, -0.0192,\n",
      "        -0.0212, -0.0213, -0.0214, -0.0211, -0.0210, -0.0210, -0.0215, -0.0217,\n",
      "        -0.0217, -0.0218, -0.0217, -0.0217, -0.0215, -0.0210, -0.0204, -0.0204,\n",
      "        -0.0202, -0.0205, -0.0207, -0.0208, -0.0207, -0.0208, -0.0208, -0.0210,\n",
      "        -0.0210, -0.0212, -0.0212, -0.0213, -0.0218, -0.0219, -0.0217, -0.0214,\n",
      "        -0.0213, -0.0214, -0.0211, -0.0209, -0.0209, -0.0212, -0.0213, -0.0214,\n",
      "        -0.0213, -0.0216, -0.0217, -0.0219, -0.0218, -0.0214, -0.0210, -0.0211,\n",
      "        -0.0209, -0.0211, -0.0210, -0.0208, -0.0210, -0.0212, -0.0210, -0.0210,\n",
      "        -0.0211, -0.0211, -0.0216, -0.0215, -0.0211, -0.0210, -0.0207, -0.0206,\n",
      "        -0.0215, -0.0209, -0.0206, -0.0205, -0.0210, -0.0212, -0.0213, -0.0217,\n",
      "        -0.0218, -0.0218, -0.0217, -0.0217, -0.0220, -0.0218, -0.0216, -0.0212,\n",
      "        -0.0210, -0.0214, -0.0218, -0.0217, -0.0220, -0.0217, -0.0216, -0.0212,\n",
      "        -0.0213, -0.0214, -0.0212, -0.0213, -0.0216, -0.0217, -0.0218, -0.0216,\n",
      "        -0.0232, -0.0234, -0.0235, -0.0231, -0.0230, -0.0231, -0.0233, -0.0231,\n",
      "        -0.0228, -0.0230, -0.0232, -0.0231, -0.0232, -0.0230, -0.0227, -0.0228,\n",
      "        -0.0227, -0.0230, -0.0227, -0.0226, -0.0226, -0.0225, -0.0225, -0.0224,\n",
      "        -0.0226, -0.0225, -0.0227, -0.0229, -0.0228, -0.0228, -0.0228, -0.0226,\n",
      "        -0.0223, -0.0223, -0.0224, -0.0222, -0.0220, -0.0218, -0.0220, -0.0221,\n",
      "        -0.0217, -0.0217, -0.0219, -0.0218, -0.0221, -0.0221, -0.0218, -0.0215,\n",
      "        -0.0213, -0.0217, -0.0214, -0.0215, -0.0216, -0.0214, -0.0214, -0.0215,\n",
      "        -0.0213, -0.0215, -0.0217, -0.0218, -0.0219, -0.0220, -0.0218, -0.0216,\n",
      "        -0.0220, -0.0218, -0.0216, -0.0213, -0.0215, -0.0215, -0.0217, -0.0215,\n",
      "        -0.0217, -0.0219, -0.0218, -0.0218, -0.0218, -0.0214, -0.0211, -0.0210,\n",
      "        -0.0208, -0.0211, -0.0211, -0.0206, -0.0208, -0.0206, -0.0202, -0.0205,\n",
      "        -0.0203, -0.0205, -0.0207, -0.0207, -0.0205, -0.0208, -0.0209, -0.0209,\n",
      "        -0.0205, -0.0201, -0.0200, -0.0196, -0.0193, -0.0196, -0.0192, -0.0192,\n",
      "        -0.0194, -0.0196, -0.0195, -0.0197, -0.0196, -0.0197, -0.0194, -0.0192,\n",
      "        -0.0190, -0.0192, -0.0193, -0.0195, -0.0195, -0.0193, -0.0195, -0.0197,\n",
      "        -0.0200, -0.0197, -0.0194, -0.0190, -0.0191, -0.0194, -0.0192, -0.0189,\n",
      "        -0.0208, -0.0209, -0.0209, -0.0210, -0.0209, -0.0206, -0.0206, -0.0212,\n",
      "        -0.0213, -0.0213, -0.0211, -0.0210, -0.0208, -0.0206, -0.0204, -0.0200,\n",
      "        -0.0198, -0.0203, -0.0203, -0.0204, -0.0205, -0.0205, -0.0204, -0.0204,\n",
      "        -0.0205, -0.0209, -0.0207, -0.0206, -0.0211, -0.0212, -0.0212, -0.0209,\n",
      "        -0.0212, -0.0207, -0.0205, -0.0203, -0.0204, -0.0204, -0.0204, -0.0207,\n",
      "        -0.0206, -0.0209, -0.0212, -0.0212, -0.0211, -0.0209, -0.0206, -0.0204,\n",
      "        -0.0201, -0.0202, -0.0203, -0.0203, -0.0203, -0.0204, -0.0203, -0.0205,\n",
      "        -0.0203, -0.0204, -0.0205, -0.0206, -0.0204, -0.0201, -0.0201, -0.0202,\n",
      "        -0.0208, -0.0210, -0.0202, -0.0203, -0.0200, -0.0206, -0.0209, -0.0213,\n",
      "        -0.0212, -0.0213, -0.0215, -0.0215, -0.0216, -0.0212, -0.0210, -0.0207,\n",
      "        -0.0205, -0.0208, -0.0212, -0.0212, -0.0212, -0.0210, -0.0209, -0.0208,\n",
      "        -0.0208, -0.0208, -0.0209, -0.0212, -0.0211, -0.0211, -0.0211, -0.0209,\n",
      "        -0.0226, -0.0225, -0.0225, -0.0224, -0.0225, -0.0227, -0.0227, -0.0229,\n",
      "        -0.0227, -0.0228, -0.0228, -0.0230, -0.0231, -0.0227, -0.0222, -0.0223,\n",
      "        -0.0221, -0.0224, -0.0225, -0.0224, -0.0222, -0.0223, -0.0222, -0.0222,\n",
      "        -0.0223, -0.0223, -0.0221, -0.0223, -0.0222, -0.0223, -0.0222, -0.0219,\n",
      "        -0.0217, -0.0218, -0.0218, -0.0215, -0.0213, -0.0212, -0.0215, -0.0216,\n",
      "        -0.0214, -0.0214, -0.0216, -0.0214, -0.0216, -0.0214, -0.0213, -0.0210,\n",
      "        -0.0206, -0.0210, -0.0210, -0.0210, -0.0209, -0.0207, -0.0206, -0.0209,\n",
      "        -0.0210, -0.0209, -0.0210, -0.0213, -0.0214, -0.0214, -0.0213, -0.0211,\n",
      "        -0.0211, -0.0215, -0.0212, -0.0211, -0.0211, -0.0210, -0.0210, -0.0214,\n",
      "        -0.0211, -0.0214, -0.0214, -0.0214, -0.0216, -0.0211, -0.0206, -0.0204,\n",
      "        -0.0202, -0.0207, -0.0205, -0.0205, -0.0204, -0.0201, -0.0199, -0.0197,\n",
      "        -0.0200, -0.0198, -0.0201, -0.0200, -0.0201, -0.0204, -0.0204, -0.0203,\n",
      "        -0.0216, -0.0218, -0.0214, -0.0210, -0.0208, -0.0209, -0.0210, -0.0208,\n",
      "        -0.0209, -0.0211, -0.0209, -0.0214, -0.0211, -0.0208, -0.0205, -0.0205,\n",
      "        -0.0208, -0.0208, -0.0210, -0.0207, -0.0208, -0.0211, -0.0211, -0.0213,\n",
      "        -0.0209, -0.0211, -0.0208, -0.0207, -0.0210, -0.0210, -0.0209, -0.0209]))]), 'optimizer_state_dict': {'state': {0: {'step': tensor(16016.), 'exp_avg': tensor([[ 2.0233e-07, -4.5059e-08, -1.2672e-07,  ..., -5.4338e-08,\n",
      "          2.1603e-08,  1.4250e-07],\n",
      "        [-6.4206e-08,  3.9940e-08, -9.7487e-08,  ...,  2.4237e-09,\n",
      "          5.8708e-08,  6.6748e-08],\n",
      "        [ 2.9261e-07,  1.3135e-07,  4.4386e-07,  ..., -3.8353e-07,\n",
      "          4.8379e-07, -4.2657e-07],\n",
      "        ...,\n",
      "        [-1.1938e-06,  1.0143e-06,  1.2596e-06,  ...,  8.4654e-07,\n",
      "          2.1871e-06,  1.6736e-07],\n",
      "        [ 3.5171e-07,  3.7366e-07,  2.2465e-07,  ..., -2.9525e-07,\n",
      "          6.7617e-07, -6.5280e-07],\n",
      "        [-8.0989e-07,  1.9715e-07, -5.1322e-07,  ..., -3.1839e-07,\n",
      "          7.1446e-07,  9.8129e-07]]), 'exp_avg_sq': tensor([[2.2593e-08, 4.1659e-09, 1.0664e-09,  ..., 5.7362e-10, 4.0164e-09,\n",
      "         1.0434e-08],\n",
      "        [2.7914e-08, 5.2668e-10, 8.7002e-10,  ..., 2.0632e-10, 9.6425e-10,\n",
      "         2.8202e-09],\n",
      "        [1.3179e-09, 1.1849e-10, 3.9841e-10,  ..., 1.8662e-10, 3.9555e-10,\n",
      "         2.3109e-10],\n",
      "        ...,\n",
      "        [1.0747e-09, 3.9333e-11, 3.6822e-11,  ..., 9.5614e-11, 1.7032e-10,\n",
      "         5.3489e-11],\n",
      "        [3.3818e-10, 4.5014e-11, 9.0335e-11,  ..., 3.2512e-11, 7.7523e-11,\n",
      "         1.3098e-10],\n",
      "        [3.3694e-09, 4.3945e-10, 4.6472e-10,  ..., 2.5693e-10, 6.4608e-10,\n",
      "         8.6942e-10]])}, 1: {'step': tensor(16016.), 'exp_avg': tensor([ 8.6430e-09,  1.0067e-08, -5.0631e-08,  ..., -1.7807e-07,\n",
      "        -5.4269e-08,  5.0342e-08]), 'exp_avg_sq': tensor([1.6512e-11, 2.4426e-11, 2.7502e-12,  ..., 1.0698e-12, 8.8897e-13,\n",
      "        6.9176e-12])}, 2: {'step': tensor(16016.), 'exp_avg': tensor([[ 2.3850e-09,  2.6534e-08,  1.1960e-07,  ...,  2.7445e-07,\n",
      "          1.3694e-07, -1.1014e-07],\n",
      "        [ 2.5039e-09,  3.3756e-09,  1.4515e-07,  ...,  3.9037e-07,\n",
      "          2.3896e-07, -9.6668e-08],\n",
      "        [ 2.9455e-09, -1.5119e-08,  1.3844e-07,  ...,  4.1203e-07,\n",
      "          2.1324e-07, -9.4813e-08],\n",
      "        ...,\n",
      "        [-7.0526e-09,  2.4951e-08,  5.7363e-07,  ...,  7.2439e-07,\n",
      "          2.1355e-06, -1.0090e-09],\n",
      "        [-6.1240e-09,  2.6285e-08,  3.4011e-07,  ...,  5.2693e-07,\n",
      "          1.1572e-06,  8.1935e-09],\n",
      "        [-7.1720e-09,  2.4351e-08,  3.6578e-07,  ...,  8.2552e-07,\n",
      "          1.3186e-06,  1.8738e-08]]), 'exp_avg_sq': tensor([[4.0607e-11, 1.4498e-11, 6.8302e-12,  ..., 3.6435e-12, 1.8443e-12,\n",
      "         9.2567e-12],\n",
      "        [4.1503e-11, 1.5035e-11, 5.0340e-12,  ..., 3.3551e-12, 2.0229e-12,\n",
      "         9.3941e-12],\n",
      "        [5.8871e-11, 1.4458e-11, 4.4626e-12,  ..., 3.5026e-12, 2.0144e-12,\n",
      "         8.5353e-12],\n",
      "        ...,\n",
      "        [3.4761e-11, 1.5923e-11, 1.1453e-11,  ..., 1.4298e-12, 3.8085e-12,\n",
      "         6.8498e-12],\n",
      "        [2.4648e-11, 1.5287e-11, 4.7687e-12,  ..., 1.1223e-12, 2.4638e-12,\n",
      "         6.7273e-12],\n",
      "        [2.5424e-11, 1.4717e-11, 3.5341e-12,  ..., 1.6561e-12, 2.6378e-12,\n",
      "         6.0849e-12]])}, 3: {'step': tensor(16016.), 'exp_avg': tensor([-8.8111e-06, -8.9295e-06, -1.2453e-05, -1.4318e-05, -1.1819e-05,\n",
      "        -1.2157e-05, -1.1994e-05, -1.4142e-05, -1.5217e-05, -1.4058e-05,\n",
      "        -1.5436e-05, -1.5407e-05, -1.2912e-05, -1.1642e-05, -1.7085e-05,\n",
      "        -1.8198e-05, -1.8698e-05, -1.8102e-05, -1.8497e-05, -1.7762e-05,\n",
      "        -1.7415e-05, -1.7415e-05, -1.7266e-05, -1.3864e-05, -1.5774e-05,\n",
      "        -1.8197e-05, -1.9896e-05, -2.2774e-05, -2.2160e-05, -2.3371e-05,\n",
      "        -2.4144e-05, -2.0165e-05, -9.0159e-06, -1.1003e-05, -1.2600e-05,\n",
      "        -1.3450e-05, -1.3304e-05, -1.4363e-05, -1.4038e-05, -1.2957e-05,\n",
      "        -1.3039e-05, -1.5255e-05, -1.4005e-05, -1.4401e-05, -1.5971e-05,\n",
      "        -1.5380e-05, -1.3858e-05, -1.6255e-05, -1.7104e-05, -1.5500e-05,\n",
      "        -1.6224e-05, -1.7071e-05, -1.7037e-05, -1.7854e-05, -1.7821e-05,\n",
      "        -1.5012e-05, -1.5052e-05, -1.7564e-05, -1.9120e-05, -1.9926e-05,\n",
      "        -1.8385e-05, -1.8450e-05, -2.0175e-05, -1.8553e-05, -1.5578e-05,\n",
      "        -1.7517e-05, -2.0025e-05, -1.7413e-05, -1.4539e-05, -1.3908e-05,\n",
      "        -1.2351e-05, -1.5686e-05, -1.6736e-05, -1.4098e-05, -1.1046e-05,\n",
      "        -9.8561e-06, -8.6214e-06, -9.0310e-06, -1.0886e-05, -1.1569e-05,\n",
      "        -1.5204e-05, -1.4431e-05, -1.6625e-05, -1.5756e-05, -1.5125e-05,\n",
      "        -1.5318e-05, -1.5735e-05, -1.6636e-05, -1.7815e-05, -1.8163e-05,\n",
      "        -1.5453e-05, -1.8220e-05, -2.0026e-05, -2.2760e-05, -2.3336e-05,\n",
      "        -1.9654e-05, -1.8060e-05, -2.0198e-05, -2.1941e-05, -1.9651e-05,\n",
      "        -1.8957e-05, -2.0640e-05, -1.8343e-05, -1.9786e-05, -1.9706e-05,\n",
      "        -2.0333e-05, -2.1802e-05, -2.2363e-05, -2.1880e-05, -2.5074e-05,\n",
      "        -2.4890e-05, -2.5969e-05, -2.6321e-05, -2.4337e-05, -2.3526e-05,\n",
      "        -2.2822e-05, -2.3776e-05, -2.3050e-05, -2.5129e-05, -2.9272e-05,\n",
      "        -2.7163e-05, -2.6769e-05, -2.7065e-05, -2.9763e-05, -2.9306e-05,\n",
      "        -2.8607e-05, -2.7288e-05, -2.4877e-05, -1.2567e-05, -1.2595e-05,\n",
      "        -1.4234e-05, -1.6325e-05, -1.7236e-05, -1.7302e-05, -1.8847e-05,\n",
      "        -2.0806e-05, -2.0205e-05, -2.0113e-05, -1.8621e-05, -1.8285e-05,\n",
      "        -1.4179e-05, -1.4371e-05, -1.6309e-05, -1.8136e-05, -1.8718e-05,\n",
      "        -1.9533e-05, -1.8926e-05, -1.9106e-05, -1.8693e-05, -1.7551e-05,\n",
      "        -1.8268e-05, -1.9975e-05, -2.3064e-05, -2.5007e-05, -2.4081e-05,\n",
      "        -2.4351e-05, -2.5501e-05, -2.6568e-05, -2.6067e-05, -2.6086e-05,\n",
      "        -1.8003e-05, -1.7178e-05, -1.9022e-05, -2.0829e-05, -2.1645e-05,\n",
      "        -2.2614e-05, -2.2504e-05, -1.9711e-05, -1.6730e-05, -1.5782e-05,\n",
      "        -1.6530e-05, -1.6653e-05, -1.5741e-05, -1.6599e-05, -1.9921e-05,\n",
      "        -1.9727e-05, -1.7829e-05, -1.6655e-05, -1.7240e-05, -1.8888e-05,\n",
      "        -1.6600e-05, -1.6383e-05, -1.6623e-05, -1.9580e-05, -2.2064e-05,\n",
      "        -2.3117e-05, -2.1737e-05, -2.1467e-05, -2.2099e-05, -2.4442e-05,\n",
      "        -2.5095e-05, -2.3396e-05, -1.6655e-05, -1.5819e-05, -1.7569e-05,\n",
      "        -2.0508e-05, -1.8537e-05, -1.7404e-05, -1.8419e-05, -1.7720e-05,\n",
      "        -1.6884e-05, -1.6343e-05, -1.5785e-05, -1.5807e-05, -1.4611e-05,\n",
      "        -1.4858e-05, -1.6829e-05, -1.7902e-05, -1.6263e-05, -1.7250e-05,\n",
      "        -1.7844e-05, -1.5871e-05, -1.3188e-05, -1.6350e-05, -1.6258e-05,\n",
      "        -1.5570e-05, -1.8365e-05, -2.1286e-05, -2.3385e-05, -2.2178e-05,\n",
      "        -2.3183e-05, -2.3602e-05, -2.3384e-05, -2.0829e-05, -6.3813e-06,\n",
      "        -6.6494e-06, -1.0168e-05, -1.2929e-05, -1.3884e-05, -1.2908e-05,\n",
      "        -1.3411e-05, -1.4386e-05, -1.3367e-05, -1.4073e-05, -1.5531e-05,\n",
      "        -1.6653e-05, -1.5333e-05, -1.6292e-05, -1.9165e-05, -2.0013e-05,\n",
      "        -2.0056e-05, -1.8751e-05, -1.7983e-05, -1.7613e-05, -1.5760e-05,\n",
      "        -1.6209e-05, -1.3418e-05, -1.2049e-05, -1.5523e-05, -1.8980e-05,\n",
      "        -1.8756e-05, -2.1852e-05, -2.2911e-05, -2.5230e-05, -2.6056e-05,\n",
      "        -2.2088e-05, -9.6040e-06, -1.0488e-05, -1.0377e-05, -1.4136e-05,\n",
      "        -1.5239e-05, -1.4312e-05, -1.4289e-05, -1.3868e-05, -1.4589e-05,\n",
      "        -1.5854e-05, -1.5226e-05, -1.6334e-05, -1.5498e-05, -1.3961e-05,\n",
      "        -1.7891e-05, -1.7240e-05, -1.8043e-05, -1.8025e-05, -1.9068e-05,\n",
      "        -1.8906e-05, -1.9035e-05, -1.7008e-05, -1.8392e-05, -1.5297e-05,\n",
      "        -1.5758e-05, -1.9047e-05, -1.9968e-05, -2.0189e-05, -1.8737e-05,\n",
      "        -1.8824e-05, -1.9538e-05, -1.7678e-05, -1.4451e-05, -1.7033e-05,\n",
      "        -1.9369e-05, -1.7507e-05, -1.5708e-05, -1.4790e-05, -1.5510e-05,\n",
      "        -1.6825e-05, -1.6216e-05, -1.3663e-05, -1.2336e-05, -1.0302e-05,\n",
      "        -8.4220e-06, -7.4080e-06, -1.0524e-05, -1.3221e-05, -1.5225e-05,\n",
      "        -1.5672e-05, -1.7099e-05, -1.5669e-05, -1.4361e-05, -1.6130e-05,\n",
      "        -1.6362e-05, -1.6115e-05, -1.7285e-05, -1.8058e-05, -1.7611e-05,\n",
      "        -1.7956e-05, -1.7932e-05, -1.9965e-05, -2.0456e-05, -2.0062e-05,\n",
      "        -1.8929e-05, -2.0642e-05, -2.1611e-05, -1.9229e-05, -1.8580e-05,\n",
      "        -1.8320e-05, -1.8229e-05, -1.9590e-05, -2.0353e-05, -2.1508e-05,\n",
      "        -2.1908e-05, -2.2412e-05, -2.2735e-05, -2.3482e-05, -2.4812e-05,\n",
      "        -2.3786e-05, -2.6239e-05, -2.4360e-05, -2.3453e-05, -2.3198e-05,\n",
      "        -2.3755e-05, -2.3505e-05, -2.3230e-05, -2.5684e-05, -2.4520e-05,\n",
      "        -2.3158e-05, -2.3852e-05, -2.8567e-05, -3.1769e-05, -3.1599e-05,\n",
      "        -2.8916e-05, -2.4756e-05, -1.3246e-05, -1.4815e-05, -1.7725e-05,\n",
      "        -1.8014e-05, -1.8771e-05, -1.9131e-05, -1.8231e-05, -1.9395e-05,\n",
      "        -1.8200e-05, -1.8468e-05, -1.8200e-05, -1.7101e-05, -1.4430e-05,\n",
      "        -1.3734e-05, -1.4413e-05, -1.7895e-05, -1.7199e-05, -1.7898e-05,\n",
      "        -1.5560e-05, -1.6143e-05, -1.7130e-05, -1.8343e-05, -1.8555e-05,\n",
      "        -2.0198e-05, -2.0912e-05, -2.4062e-05, -2.5197e-05, -2.5567e-05,\n",
      "        -2.5155e-05, -2.6761e-05, -2.5413e-05, -2.4250e-05, -1.7847e-05,\n",
      "        -1.8135e-05, -1.7396e-05, -1.8765e-05, -2.2750e-05, -2.3916e-05,\n",
      "        -2.2693e-05, -2.1449e-05, -1.7959e-05, -1.6432e-05, -1.8353e-05,\n",
      "        -1.9076e-05, -1.6517e-05, -1.8059e-05, -2.0957e-05, -2.1551e-05,\n",
      "        -1.8918e-05, -1.5843e-05, -1.5792e-05, -1.5730e-05, -1.5452e-05,\n",
      "        -1.5132e-05, -1.6813e-05, -2.0028e-05, -2.1691e-05, -2.0670e-05,\n",
      "        -1.9352e-05, -2.0675e-05, -2.2587e-05, -2.3464e-05, -2.6092e-05,\n",
      "        -2.3950e-05, -1.8609e-05, -1.9397e-05, -2.1657e-05, -2.2336e-05,\n",
      "        -2.0995e-05, -1.9425e-05, -2.0963e-05, -1.9197e-05, -1.5088e-05,\n",
      "        -1.5603e-05, -1.6581e-05, -1.7282e-05, -1.4063e-05, -1.4114e-05,\n",
      "        -1.6276e-05, -1.7788e-05, -1.8357e-05, -1.9825e-05, -1.8476e-05,\n",
      "        -1.6545e-05, -1.3664e-05, -1.6021e-05, -1.7235e-05, -1.8572e-05,\n",
      "        -2.1665e-05, -2.1144e-05, -2.2926e-05, -2.2508e-05, -2.1148e-05,\n",
      "        -2.2197e-05, -2.3265e-05, -2.0826e-05, -8.3861e-06, -7.8705e-06,\n",
      "        -1.0478e-05, -1.4141e-05, -1.4104e-05, -1.3452e-05, -1.3935e-05,\n",
      "        -1.5643e-05, -1.3961e-05, -1.4610e-05, -1.5962e-05, -1.5290e-05,\n",
      "        -1.5195e-05, -1.7906e-05, -1.9345e-05, -2.0528e-05, -1.9837e-05,\n",
      "        -1.9125e-05, -1.6253e-05, -1.6160e-05, -1.4441e-05, -1.5605e-05,\n",
      "        -1.3869e-05, -1.2519e-05, -1.8925e-05, -2.1214e-05, -2.2570e-05,\n",
      "        -2.3792e-05, -2.5531e-05, -2.6053e-05, -2.6819e-05, -2.4177e-05,\n",
      "        -1.0776e-05, -8.9617e-06, -1.0265e-05, -1.4596e-05, -1.4856e-05,\n",
      "        -1.2975e-05, -1.4258e-05, -1.4600e-05, -1.6373e-05, -1.6386e-05,\n",
      "        -1.6281e-05, -1.9312e-05, -1.8237e-05, -1.6422e-05, -1.6029e-05,\n",
      "        -1.7458e-05, -1.9081e-05, -1.8423e-05, -1.9431e-05, -2.0014e-05,\n",
      "        -2.0225e-05, -1.9412e-05, -1.9146e-05, -1.7002e-05, -1.7346e-05,\n",
      "        -1.9807e-05, -1.9227e-05, -1.8984e-05, -1.9283e-05, -1.9321e-05,\n",
      "        -1.9953e-05, -1.8874e-05, -1.3734e-05, -1.5876e-05, -1.6813e-05,\n",
      "        -1.6566e-05, -1.3302e-05, -1.3305e-05, -1.4672e-05, -1.7022e-05,\n",
      "        -1.7476e-05, -1.5157e-05, -1.1661e-05, -1.1552e-05, -7.9668e-06,\n",
      "        -6.2070e-06, -1.0207e-05, -1.2874e-05, -1.5398e-05, -1.5931e-05,\n",
      "        -1.6026e-05, -1.5641e-05, -1.4783e-05, -1.4959e-05, -1.6413e-05,\n",
      "        -1.4856e-05, -1.7090e-05, -1.9347e-05, -1.9633e-05, -1.9882e-05,\n",
      "        -1.7408e-05, -1.8999e-05, -2.0882e-05, -1.8613e-05, -1.5857e-05,\n",
      "        -1.8182e-05, -1.9556e-05, -1.9463e-05, -1.8978e-05, -1.9023e-05,\n",
      "        -1.7843e-05, -1.8872e-05, -2.0458e-05, -2.1426e-05, -2.0878e-05,\n",
      "        -2.0233e-05, -2.2343e-05, -2.2279e-05, -2.2811e-05, -2.3369e-05,\n",
      "        -2.4793e-05, -2.4141e-05, -2.4429e-05, -2.5150e-05, -2.4082e-05,\n",
      "        -2.3922e-05, -2.2670e-05, -2.2174e-05, -2.4324e-05, -2.4622e-05,\n",
      "        -2.4652e-05, -2.7423e-05, -2.7658e-05, -2.8737e-05, -2.9073e-05,\n",
      "        -2.5705e-05, -1.3775e-05, -1.3409e-05, -1.6232e-05, -1.9262e-05,\n",
      "        -1.9495e-05, -1.7701e-05, -1.6416e-05, -1.7431e-05, -1.7201e-05,\n",
      "        -1.7605e-05, -1.5187e-05, -1.4707e-05, -1.4614e-05, -1.5571e-05,\n",
      "        -1.5899e-05, -1.6553e-05, -1.7446e-05, -1.3482e-05, -1.5882e-05,\n",
      "        -1.7438e-05, -1.7837e-05, -1.8668e-05, -1.8811e-05, -1.9976e-05,\n",
      "        -2.0120e-05, -2.2099e-05, -2.2843e-05, -2.4496e-05, -2.5259e-05,\n",
      "        -2.6454e-05, -2.5344e-05, -2.3027e-05, -1.5901e-05, -1.7547e-05,\n",
      "        -1.8647e-05, -2.0448e-05, -2.1624e-05, -2.0037e-05, -1.8422e-05,\n",
      "        -1.9982e-05, -1.8963e-05, -1.6972e-05, -1.8627e-05, -1.9775e-05,\n",
      "        -1.7625e-05, -1.6385e-05, -1.7784e-05, -1.7741e-05, -1.6283e-05,\n",
      "        -1.2516e-05, -1.0839e-05, -1.1340e-05, -1.3477e-05, -1.5346e-05,\n",
      "        -1.6185e-05, -1.8719e-05, -1.8945e-05, -1.9304e-05, -1.7376e-05,\n",
      "        -1.9787e-05, -2.1470e-05, -2.4026e-05, -2.5044e-05, -2.1459e-05,\n",
      "        -1.7292e-05, -2.1063e-05, -2.1699e-05, -2.0296e-05, -1.9019e-05,\n",
      "        -1.7690e-05, -1.9360e-05, -1.8424e-05, -1.4344e-05, -1.4113e-05,\n",
      "        -1.6181e-05, -1.7050e-05, -1.5009e-05, -1.4577e-05, -1.6046e-05,\n",
      "        -1.7083e-05, -1.7846e-05, -1.8083e-05, -1.7786e-05, -1.6337e-05,\n",
      "        -1.5711e-05, -1.4549e-05, -1.5869e-05, -1.6673e-05, -1.9005e-05,\n",
      "        -1.7615e-05, -2.0200e-05, -2.2560e-05, -2.0228e-05, -2.0252e-05,\n",
      "        -2.0948e-05, -1.9251e-05, -8.6363e-06, -8.5219e-06, -1.0758e-05,\n",
      "        -1.2729e-05, -1.3195e-05, -1.3229e-05, -1.4215e-05, -1.5311e-05,\n",
      "        -1.5195e-05, -1.5474e-05, -1.4998e-05, -1.4690e-05, -1.4163e-05,\n",
      "        -1.6146e-05, -2.0454e-05, -1.9386e-05, -1.6797e-05, -1.7328e-05,\n",
      "        -1.4897e-05, -1.4667e-05, -1.3299e-05, -1.4253e-05, -1.3302e-05,\n",
      "        -1.5637e-05, -2.0125e-05, -2.1129e-05, -2.3194e-05, -2.2921e-05,\n",
      "        -2.3079e-05, -2.3833e-05, -2.6879e-05, -2.4954e-05, -1.1082e-05,\n",
      "        -1.0980e-05, -1.0628e-05, -1.6058e-05, -1.5231e-05, -1.3283e-05,\n",
      "        -1.5785e-05, -1.5626e-05, -1.4129e-05, -1.6133e-05, -1.5630e-05,\n",
      "        -1.7063e-05, -1.5776e-05, -1.5526e-05, -1.6127e-05, -1.8077e-05,\n",
      "        -1.7331e-05, -1.7682e-05, -1.7588e-05, -1.9458e-05, -1.8287e-05,\n",
      "        -1.8288e-05, -1.7305e-05, -1.5016e-05, -1.7590e-05, -1.7930e-05,\n",
      "        -1.7077e-05, -1.8188e-05, -2.0015e-05, -2.0339e-05, -1.9749e-05,\n",
      "        -1.9211e-05, -1.3020e-05, -1.4004e-05, -1.5350e-05, -1.6369e-05,\n",
      "        -1.4096e-05, -1.3503e-05, -1.4402e-05, -1.5872e-05, -1.7756e-05,\n",
      "        -1.5790e-05, -1.3636e-05, -1.4180e-05, -1.0620e-05, -8.8531e-06,\n",
      "        -1.0135e-05, -1.4209e-05, -1.6871e-05, -1.5295e-05, -1.6064e-05,\n",
      "        -1.6720e-05, -1.6367e-05, -1.7074e-05, -1.6643e-05, -1.4748e-05,\n",
      "        -1.5951e-05, -1.8689e-05, -1.7236e-05, -1.9794e-05, -1.8513e-05,\n",
      "        -1.8400e-05, -2.1201e-05, -2.0551e-05]), 'exp_avg_sq': tensor([1.2243e-07, 1.2226e-07, 1.2235e-07, 1.2256e-07, 1.2300e-07, 1.2336e-07,\n",
      "        1.2363e-07, 1.2400e-07, 1.2414e-07, 1.2414e-07, 1.2430e-07, 1.2405e-07,\n",
      "        1.2404e-07, 1.2396e-07, 1.2371e-07, 1.2355e-07, 1.2333e-07, 1.2318e-07,\n",
      "        1.2280e-07, 1.2293e-07, 1.2306e-07, 1.2295e-07, 1.2284e-07, 1.2272e-07,\n",
      "        1.2286e-07, 1.2319e-07, 1.2306e-07, 1.2286e-07, 1.2272e-07, 1.2285e-07,\n",
      "        1.2313e-07, 1.2351e-07, 1.2261e-07, 1.2252e-07, 1.2254e-07, 1.2254e-07,\n",
      "        1.2295e-07, 1.2307e-07, 1.2322e-07, 1.2353e-07, 1.2376e-07, 1.2360e-07,\n",
      "        1.2356e-07, 1.2365e-07, 1.2327e-07, 1.2301e-07, 1.2286e-07, 1.2286e-07,\n",
      "        1.2281e-07, 1.2268e-07, 1.2282e-07, 1.2278e-07, 1.2284e-07, 1.2292e-07,\n",
      "        1.2292e-07, 1.2298e-07, 1.2307e-07, 1.2343e-07, 1.2358e-07, 1.2354e-07,\n",
      "        1.2355e-07, 1.2310e-07, 1.2384e-07, 1.2401e-07, 1.2321e-07, 1.2302e-07,\n",
      "        1.2268e-07, 1.2298e-07, 1.2309e-07, 1.2291e-07, 1.2275e-07, 1.2266e-07,\n",
      "        1.2294e-07, 1.2256e-07, 1.2271e-07, 1.2275e-07, 1.2290e-07, 1.2282e-07,\n",
      "        1.2276e-07, 1.2271e-07, 1.2279e-07, 1.2302e-07, 1.2283e-07, 1.2281e-07,\n",
      "        1.2262e-07, 1.2268e-07, 1.2262e-07, 1.2283e-07, 1.2275e-07, 1.2269e-07,\n",
      "        1.2280e-07, 1.2295e-07, 1.2240e-07, 1.2257e-07, 1.2300e-07, 1.2368e-07,\n",
      "        1.2334e-07, 1.2300e-07, 1.2323e-07, 1.2400e-07, 1.2401e-07, 1.2408e-07,\n",
      "        1.2426e-07, 1.2431e-07, 1.2464e-07, 1.2423e-07, 1.2404e-07, 1.2436e-07,\n",
      "        1.2415e-07, 1.2354e-07, 1.2403e-07, 1.2404e-07, 1.2371e-07, 1.2351e-07,\n",
      "        1.2361e-07, 1.2346e-07, 1.2374e-07, 1.2327e-07, 1.2343e-07, 1.2379e-07,\n",
      "        1.2388e-07, 1.2378e-07, 1.2337e-07, 1.2346e-07, 1.2325e-07, 1.2322e-07,\n",
      "        1.2370e-07, 1.2458e-07, 1.2391e-07, 1.2341e-07, 1.2374e-07, 1.2379e-07,\n",
      "        1.2391e-07, 1.2411e-07, 1.2422e-07, 1.2430e-07, 1.2407e-07, 1.2386e-07,\n",
      "        1.2356e-07, 1.2350e-07, 1.2339e-07, 1.2327e-07, 1.2314e-07, 1.2308e-07,\n",
      "        1.2343e-07, 1.2358e-07, 1.2365e-07, 1.2389e-07, 1.2404e-07, 1.2376e-07,\n",
      "        1.2362e-07, 1.2405e-07, 1.2452e-07, 1.2412e-07, 1.2392e-07, 1.2385e-07,\n",
      "        1.2387e-07, 1.2376e-07, 1.2412e-07, 1.2420e-07, 1.2307e-07, 1.2307e-07,\n",
      "        1.2287e-07, 1.2276e-07, 1.2268e-07, 1.2288e-07, 1.2292e-07, 1.2298e-07,\n",
      "        1.2291e-07, 1.2276e-07, 1.2288e-07, 1.2272e-07, 1.2277e-07, 1.2347e-07,\n",
      "        1.2352e-07, 1.2357e-07, 1.2360e-07, 1.2368e-07, 1.2393e-07, 1.2362e-07,\n",
      "        1.2361e-07, 1.2336e-07, 1.2341e-07, 1.2378e-07, 1.2344e-07, 1.2359e-07,\n",
      "        1.2288e-07, 1.2257e-07, 1.2250e-07, 1.2274e-07, 1.2326e-07, 1.2348e-07,\n",
      "        1.2239e-07, 1.2244e-07, 1.2268e-07, 1.2329e-07, 1.2356e-07, 1.2345e-07,\n",
      "        1.2377e-07, 1.2396e-07, 1.2378e-07, 1.2333e-07, 1.2290e-07, 1.2267e-07,\n",
      "        1.2271e-07, 1.2320e-07, 1.2350e-07, 1.2349e-07, 1.2327e-07, 1.2310e-07,\n",
      "        1.2337e-07, 1.2333e-07, 1.2309e-07, 1.2292e-07, 1.2309e-07, 1.2323e-07,\n",
      "        1.2312e-07, 1.2301e-07, 1.2292e-07, 1.2273e-07, 1.2282e-07, 1.2321e-07,\n",
      "        1.2351e-07, 1.2351e-07, 1.2202e-07, 1.2197e-07, 1.2227e-07, 1.2245e-07,\n",
      "        1.2291e-07, 1.2341e-07, 1.2352e-07, 1.2385e-07, 1.2420e-07, 1.2422e-07,\n",
      "        1.2425e-07, 1.2404e-07, 1.2373e-07, 1.2375e-07, 1.2353e-07, 1.2334e-07,\n",
      "        1.2301e-07, 1.2294e-07, 1.2286e-07, 1.2289e-07, 1.2288e-07, 1.2269e-07,\n",
      "        1.2247e-07, 1.2266e-07, 1.2290e-07, 1.2288e-07, 1.2280e-07, 1.2273e-07,\n",
      "        1.2258e-07, 1.2306e-07, 1.2314e-07, 1.2318e-07, 1.2269e-07, 1.2265e-07,\n",
      "        1.2300e-07, 1.2281e-07, 1.2311e-07, 1.2329e-07, 1.2322e-07, 1.2357e-07,\n",
      "        1.2390e-07, 1.2397e-07, 1.2363e-07, 1.2346e-07, 1.2323e-07, 1.2321e-07,\n",
      "        1.2324e-07, 1.2313e-07, 1.2296e-07, 1.2267e-07, 1.2280e-07, 1.2277e-07,\n",
      "        1.2279e-07, 1.2298e-07, 1.2298e-07, 1.2311e-07, 1.2311e-07, 1.2341e-07,\n",
      "        1.2345e-07, 1.2324e-07, 1.2322e-07, 1.2344e-07, 1.2386e-07, 1.2417e-07,\n",
      "        1.2333e-07, 1.2315e-07, 1.2313e-07, 1.2304e-07, 1.2296e-07, 1.2289e-07,\n",
      "        1.2285e-07, 1.2282e-07, 1.2296e-07, 1.2258e-07, 1.2257e-07, 1.2292e-07,\n",
      "        1.2277e-07, 1.2262e-07, 1.2247e-07, 1.2257e-07, 1.2278e-07, 1.2256e-07,\n",
      "        1.2232e-07, 1.2280e-07, 1.2256e-07, 1.2282e-07, 1.2296e-07, 1.2261e-07,\n",
      "        1.2260e-07, 1.2285e-07, 1.2303e-07, 1.2315e-07, 1.2258e-07, 1.2273e-07,\n",
      "        1.2288e-07, 1.2330e-07, 1.2314e-07, 1.2297e-07, 1.2309e-07, 1.2346e-07,\n",
      "        1.2397e-07, 1.2411e-07, 1.2435e-07, 1.2457e-07, 1.2471e-07, 1.2438e-07,\n",
      "        1.2412e-07, 1.2410e-07, 1.2408e-07, 1.2405e-07, 1.2393e-07, 1.2400e-07,\n",
      "        1.2383e-07, 1.2329e-07, 1.2352e-07, 1.2384e-07, 1.2347e-07, 1.2360e-07,\n",
      "        1.2366e-07, 1.2387e-07, 1.2375e-07, 1.2357e-07, 1.2327e-07, 1.2309e-07,\n",
      "        1.2300e-07, 1.2323e-07, 1.2339e-07, 1.2396e-07, 1.2323e-07, 1.2315e-07,\n",
      "        1.2357e-07, 1.2337e-07, 1.2331e-07, 1.2369e-07, 1.2376e-07, 1.2386e-07,\n",
      "        1.2396e-07, 1.2352e-07, 1.2327e-07, 1.2375e-07, 1.2347e-07, 1.2296e-07,\n",
      "        1.2297e-07, 1.2308e-07, 1.2325e-07, 1.2342e-07, 1.2365e-07, 1.2387e-07,\n",
      "        1.2400e-07, 1.2385e-07, 1.2365e-07, 1.2386e-07, 1.2406e-07, 1.2413e-07,\n",
      "        1.2397e-07, 1.2373e-07, 1.2352e-07, 1.2312e-07, 1.2370e-07, 1.2424e-07,\n",
      "        1.2354e-07, 1.2318e-07, 1.2317e-07, 1.2291e-07, 1.2284e-07, 1.2293e-07,\n",
      "        1.2316e-07, 1.2299e-07, 1.2302e-07, 1.2283e-07, 1.2283e-07, 1.2293e-07,\n",
      "        1.2266e-07, 1.2298e-07, 1.2333e-07, 1.2323e-07, 1.2299e-07, 1.2318e-07,\n",
      "        1.2354e-07, 1.2361e-07, 1.2332e-07, 1.2329e-07, 1.2305e-07, 1.2324e-07,\n",
      "        1.2309e-07, 1.2297e-07, 1.2263e-07, 1.2260e-07, 1.2280e-07, 1.2250e-07,\n",
      "        1.2338e-07, 1.2344e-07, 1.2260e-07, 1.2233e-07, 1.2259e-07, 1.2316e-07,\n",
      "        1.2348e-07, 1.2325e-07, 1.2373e-07, 1.2382e-07, 1.2387e-07, 1.2330e-07,\n",
      "        1.2307e-07, 1.2292e-07, 1.2296e-07, 1.2309e-07, 1.2345e-07, 1.2359e-07,\n",
      "        1.2337e-07, 1.2313e-07, 1.2334e-07, 1.2321e-07, 1.2335e-07, 1.2313e-07,\n",
      "        1.2323e-07, 1.2307e-07, 1.2309e-07, 1.2290e-07, 1.2283e-07, 1.2267e-07,\n",
      "        1.2268e-07, 1.2309e-07, 1.2352e-07, 1.2355e-07, 1.2235e-07, 1.2233e-07,\n",
      "        1.2220e-07, 1.2251e-07, 1.2307e-07, 1.2330e-07, 1.2369e-07, 1.2420e-07,\n",
      "        1.2435e-07, 1.2413e-07, 1.2390e-07, 1.2398e-07, 1.2373e-07, 1.2363e-07,\n",
      "        1.2338e-07, 1.2328e-07, 1.2318e-07, 1.2321e-07, 1.2324e-07, 1.2317e-07,\n",
      "        1.2290e-07, 1.2279e-07, 1.2255e-07, 1.2299e-07, 1.2289e-07, 1.2283e-07,\n",
      "        1.2281e-07, 1.2257e-07, 1.2250e-07, 1.2307e-07, 1.2329e-07, 1.2340e-07,\n",
      "        1.2260e-07, 1.2245e-07, 1.2257e-07, 1.2272e-07, 1.2296e-07, 1.2301e-07,\n",
      "        1.2305e-07, 1.2360e-07, 1.2381e-07, 1.2379e-07, 1.2334e-07, 1.2321e-07,\n",
      "        1.2314e-07, 1.2321e-07, 1.2322e-07, 1.2315e-07, 1.2290e-07, 1.2273e-07,\n",
      "        1.2277e-07, 1.2279e-07, 1.2315e-07, 1.2319e-07, 1.2285e-07, 1.2303e-07,\n",
      "        1.2320e-07, 1.2316e-07, 1.2297e-07, 1.2283e-07, 1.2253e-07, 1.2307e-07,\n",
      "        1.2377e-07, 1.2407e-07, 1.2378e-07, 1.2374e-07, 1.2365e-07, 1.2367e-07,\n",
      "        1.2350e-07, 1.2337e-07, 1.2327e-07, 1.2312e-07, 1.2301e-07, 1.2299e-07,\n",
      "        1.2295e-07, 1.2295e-07, 1.2295e-07, 1.2308e-07, 1.2289e-07, 1.2282e-07,\n",
      "        1.2275e-07, 1.2258e-07, 1.2267e-07, 1.2290e-07, 1.2319e-07, 1.2315e-07,\n",
      "        1.2330e-07, 1.2323e-07, 1.2324e-07, 1.2336e-07, 1.2317e-07, 1.2321e-07,\n",
      "        1.2305e-07, 1.2321e-07, 1.2365e-07, 1.2413e-07, 1.2386e-07, 1.2353e-07,\n",
      "        1.2341e-07, 1.2359e-07, 1.2424e-07, 1.2429e-07, 1.2448e-07, 1.2493e-07,\n",
      "        1.2498e-07, 1.2464e-07, 1.2432e-07, 1.2408e-07, 1.2442e-07, 1.2415e-07,\n",
      "        1.2439e-07, 1.2434e-07, 1.2396e-07, 1.2380e-07, 1.2370e-07, 1.2383e-07,\n",
      "        1.2381e-07, 1.2368e-07, 1.2383e-07, 1.2385e-07, 1.2417e-07, 1.2382e-07,\n",
      "        1.2400e-07, 1.2333e-07, 1.2323e-07, 1.2306e-07, 1.2339e-07, 1.2377e-07,\n",
      "        1.2300e-07, 1.2288e-07, 1.2297e-07, 1.2320e-07, 1.2337e-07, 1.2342e-07,\n",
      "        1.2390e-07, 1.2416e-07, 1.2409e-07, 1.2381e-07, 1.2394e-07, 1.2392e-07,\n",
      "        1.2370e-07, 1.2326e-07, 1.2337e-07, 1.2320e-07, 1.2352e-07, 1.2371e-07,\n",
      "        1.2404e-07, 1.2422e-07, 1.2421e-07, 1.2380e-07, 1.2407e-07, 1.2396e-07,\n",
      "        1.2369e-07, 1.2371e-07, 1.2361e-07, 1.2358e-07, 1.2330e-07, 1.2336e-07,\n",
      "        1.2395e-07, 1.2431e-07, 1.2366e-07, 1.2330e-07, 1.2332e-07, 1.2303e-07,\n",
      "        1.2289e-07, 1.2286e-07, 1.2303e-07, 1.2311e-07, 1.2306e-07, 1.2287e-07,\n",
      "        1.2297e-07, 1.2314e-07, 1.2334e-07, 1.2332e-07, 1.2327e-07, 1.2318e-07,\n",
      "        1.2309e-07, 1.2348e-07, 1.2369e-07, 1.2297e-07, 1.2308e-07, 1.2311e-07,\n",
      "        1.2301e-07, 1.2316e-07, 1.2306e-07, 1.2299e-07, 1.2300e-07, 1.2242e-07,\n",
      "        1.2244e-07, 1.2251e-07, 1.2311e-07, 1.2373e-07, 1.2280e-07, 1.2258e-07,\n",
      "        1.2301e-07, 1.2326e-07, 1.2332e-07, 1.2336e-07, 1.2355e-07, 1.2401e-07,\n",
      "        1.2403e-07, 1.2359e-07, 1.2339e-07, 1.2340e-07, 1.2322e-07, 1.2296e-07,\n",
      "        1.2348e-07, 1.2346e-07, 1.2344e-07, 1.2317e-07, 1.2343e-07, 1.2339e-07,\n",
      "        1.2330e-07, 1.2351e-07, 1.2343e-07, 1.2361e-07, 1.2325e-07, 1.2321e-07,\n",
      "        1.2290e-07, 1.2278e-07, 1.2309e-07, 1.2337e-07, 1.2374e-07, 1.2401e-07,\n",
      "        1.2294e-07, 1.2267e-07, 1.2276e-07, 1.2318e-07, 1.2346e-07, 1.2360e-07,\n",
      "        1.2384e-07, 1.2426e-07, 1.2433e-07, 1.2415e-07, 1.2372e-07, 1.2388e-07,\n",
      "        1.2351e-07, 1.2324e-07, 1.2289e-07, 1.2296e-07, 1.2318e-07, 1.2326e-07,\n",
      "        1.2310e-07, 1.2304e-07, 1.2277e-07, 1.2293e-07, 1.2298e-07, 1.2315e-07,\n",
      "        1.2318e-07, 1.2312e-07, 1.2281e-07, 1.2280e-07, 1.2271e-07, 1.2321e-07,\n",
      "        1.2371e-07, 1.2426e-07, 1.2285e-07, 1.2281e-07, 1.2289e-07, 1.2275e-07,\n",
      "        1.2315e-07, 1.2325e-07, 1.2357e-07, 1.2365e-07, 1.2378e-07, 1.2372e-07,\n",
      "        1.2341e-07, 1.2316e-07, 1.2329e-07, 1.2356e-07, 1.2354e-07, 1.2366e-07,\n",
      "        1.2349e-07, 1.2360e-07, 1.2364e-07, 1.2366e-07, 1.2323e-07, 1.2341e-07,\n",
      "        1.2322e-07, 1.2333e-07, 1.2353e-07, 1.2331e-07, 1.2340e-07, 1.2309e-07,\n",
      "        1.2310e-07, 1.2345e-07, 1.2404e-07, 1.2406e-07, 1.2355e-07, 1.2353e-07,\n",
      "        1.2358e-07, 1.2343e-07, 1.2337e-07, 1.2355e-07, 1.2353e-07, 1.2306e-07,\n",
      "        1.2277e-07, 1.2264e-07, 1.2258e-07, 1.2272e-07, 1.2261e-07, 1.2296e-07,\n",
      "        1.2309e-07, 1.2309e-07, 1.2283e-07, 1.2281e-07, 1.2264e-07, 1.2278e-07,\n",
      "        1.2259e-07, 1.2258e-07, 1.2277e-07, 1.2300e-07, 1.2324e-07, 1.2328e-07,\n",
      "        1.2310e-07, 1.2282e-07, 1.2282e-07, 1.2342e-07, 1.2371e-07, 1.2394e-07])}}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3]}]}, 'loss': tensor(1.3951, requires_grad=True), 'val_loss': 1.3794373347254223, 'checkpoint_type': 'recon'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f5/0wsbjcm56b5czjl3m5v9l1tm0000gn/T/ipykernel_4705/3310458154.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  recon_checkpoint =torch.load(f'checkpoints/recon_checkpoints/best_checkpoint.pth', map_location=torch.device(device))\n"
     ]
    }
   ],
   "source": [
    "#if the model is pretrained, this checkpoint can be used and the training can be skipped\n",
    "recon_checkpoint =torch.load(f'checkpoints/recon_checkpoints/best_checkpoint.pth', map_location=torch.device(device))\n",
    "print('epoch', recon_checkpoint['epoch'])\n",
    "\n",
    "recon_model = Mlp(in_features=384, hidden_features=1536, out_features=3*16*16\n",
    "        )\n",
    "print(recon_checkpoint.keys())\n",
    "recon_model.load_state_dict(recon_checkpoint['model_state_dict'])\n",
    "recon_model.to(device)\n",
    "print(recon_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_error(model, transformer, data):\n",
    "    model.eval()\n",
    "    errors = []\n",
    "\n",
    "    for images, _ in tqdm(data, desc=\"Calculating L2 error\"):\n",
    "        images = images.to(device)\n",
    "        B = images.shape[0]\n",
    "        data_inputs = images.reshape(B, 196, 768)\n",
    "\n",
    "        x = transformer.patch_embed(images)\n",
    "        cls_tokens = transformer.cls_token.expand(B, -1, -1)\n",
    "        x = x + transformer.pos_embed\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        for blk in transformer.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        preds = model(x[:, 1:, :])\n",
    "        preds = preds.squeeze(dim=1)\n",
    "\n",
    "        error = torch.norm(preds - data_inputs, dim=1).mean()\n",
    "        errors.append(error.item())\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, transformer, optimizer, train_data, val_data, loss_module, num_epochs=100, patience=10):\n",
    "    # check if checkpoint exists\n",
    "    checkpoint_loaded = False\n",
    "    if os.path.exists('checkpoints/recon_checkpoints/checkpoint.pth'):\n",
    "        checkpoint = torch.load('checkpoints/recon_checkpoints/checkpoint.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        checkpoint_loaded = True\n",
    "        print('Loaded checkpoint')\n",
    "        if checkpoint['epoch']+1 >= num_epochs:\n",
    "            print('Model already trained for', num_epochs, 'epochs.')\n",
    "            return model\n",
    "        else:\n",
    "            print('Model has been trained for', checkpoint['epoch']+1, 'epochs.')\n",
    "            num_epochs -= checkpoint['epoch']+1\n",
    "\n",
    "    # Set the model to training mode.\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    if checkpoint_loaded:\n",
    "        best_loss = checkpoint['val_loss']\n",
    "    else:\n",
    "        best_loss = float('inf')\n",
    "    best_model_params = None\n",
    "\n",
    "    no_improvement = 0\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        if checkpoint_loaded and epoch == 0:\n",
    "            epoch += checkpoint['epoch']+1\n",
    "        \n",
    "        for images, _ in train_data:  # New DataLoader returns (images, labels); we ignore labels if not needed.\n",
    "            images = images.to(device)\n",
    "            B = images.shape[0]  # Actual batch size (may vary for the last batch).\n",
    "\n",
    "            # Optionally, reshape images for use as targets in the loss.\n",
    "            # (Using B rather than a fixed batch_size ensures compatibility if the last batch is smaller.)\n",
    "            data_inputs = images.reshape(B, 196, 768)\n",
    "\n",
    "            # Pass images through the transformer pipeline.\n",
    "            x = transformer.patch_embed(images)\n",
    "            cls_tokens = transformer.cls_token.expand(B, -1, -1)\n",
    "            x = x + transformer.pos_embed\n",
    "            x = torch.cat((cls_tokens, x), dim=1)\n",
    "            for blk in transformer.blocks:\n",
    "                x = blk(x)\n",
    "\n",
    "            # Forward pass through the model (skipping the class token).\n",
    "            preds = model(x[:, 1:, :])\n",
    "            preds = preds.squeeze(dim=1)  # Change shape from [B, 1] to [B].\n",
    "\n",
    "            # Compute the loss.\n",
    "            loss = loss_module(preds, data_inputs).to(device)\n",
    "\n",
    "            # Backpropagation steps.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = l2_error(model, transformer, val_data)\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "        print(epoch)\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,  # current epoch number\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,  # optional: current loss value\n",
    "            'val_loss': val_loss,  # optional: current validation loss\n",
    "            'checkpoint_type': 'recon'  # identifier for the recon model\n",
    "        }\n",
    "\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}, Validation Loss = {val_loss}\")\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            no_improvement = 0\n",
    "            best_loss = val_loss\n",
    "            best_model_params = model.state_dict()  # Save best model parameters.\n",
    "\n",
    "            torch.save(checkpoint, f'checkpoints/recon_checkpoints/best_checkpoint.pth')\n",
    "            print('New best loss achieved.')\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        torch.save(checkpoint, f'checkpoints/recon_checkpoints/checkpoint.pth')\n",
    "    \n",
    "        if no_improvement >= patience:\n",
    "            print('No improvement for', patience, 'epochs. Stopping training.')\n",
    "            break\n",
    "\n",
    "    return best_model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(recon_model.parameters())\n",
    "loss = nn.MSELoss()\n",
    "train_model(model=recon_model, transformer= model_deit, optimizer=optimizer, train_data=train_loader, val_data=val_loader, loss_module=loss, num_epochs=200, patience=5)\n",
    "\n",
    "# clear cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best recon model\n",
    "checkpoint = torch.load('checkpoints/recon_checkpoints/best_checkpoint.pth')\n",
    "recon_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "recon_model.to(device)\n",
    "\n",
    "errors = l2_error(recon_model, model_deit, val_loader)\n",
    "print(f'Mean L2 error: {np.mean(errors)}')\n",
    "print(f'Median L2 error: {np.median(errors)}')\n",
    "print(f'Min L2 error: {np.min(errors)}')\n",
    "print(f'Max L2 error: {np.max(errors)}')\n",
    "\n",
    "# clear cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comparing high and low norm patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting norm threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deit_final_patches(model, imgs):\n",
    "    \"\"\"\n",
    "    For DeiT-III, replicate a forward pass to get final patch tokens (excluding the CLS token).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(imgs)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # add cls token + pos embed\n",
    "        cls_token = model.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        pos_embed = model.pos_embed[:, : (N+1), :]\n",
    "        x = torch.cat([cls_token, x], dim=1)  # => [B, N+1, D]\n",
    "        x = x[:, 1:, :] + pos_embed\n",
    "\n",
    "        for blk in model.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = model.norm(x)  # [B, N+1, D]\n",
    "        x = x  # => [B, N, D]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_global_threshold(\n",
    "    final_extraction_func,\n",
    "    model,\n",
    "    loader,\n",
    "    percentile=0.98\n",
    "):\n",
    "    all_norms = []\n",
    "    for i, (imgs, _) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        final_embs = final_extraction_func(model, imgs)  # [B, N, D]\n",
    "        norms = torch.norm(final_embs, dim=-1)  # [B, N]\n",
    "        all_norms.append(norms.flatten().cpu())\n",
    "    all_norms = torch.cat(all_norms, dim=0)\n",
    "    threshold = torch.quantile(all_norms, percentile).item()\n",
    "    return threshold\n",
    "\n",
    "#print(gather_global_threshold(get_deit_final_patches, model_deit, val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather patches with high norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm threshold: 72.4803695678711\n",
      "High norm error: 176.54341995716095\n",
      "Low norm error: 120.02065622806549\n",
      "ratio: 147.0941965370443\n"
     ]
    }
   ],
   "source": [
    "# To investigate the local information that high and low norm patches hold, they are separated and the loss on them is calculated and compared\n",
    "high_norm_patches_error = 0\n",
    "low_norm_patches_error = 0\n",
    "i = 0\n",
    "high_norm_patches_errors = []\n",
    "low_norm_patches_errors = []\n",
    "batch_size = val_loader.batch_size\n",
    "threshold = gather_global_threshold(get_deit_final_patches, model_deit, val_loader)\n",
    "print(\"Norm threshold:\", threshold)\n",
    "\n",
    "for imgs, _ in val_loader:\n",
    "    final_embs = get_deit_final_patches(model_deit, imgs)\n",
    "    norms = torch.norm(final_embs, dim=-1)  # [B, N]\n",
    "    norms = (norms >= threshold)\n",
    "\n",
    "    sample_images = imgs.permute(0, 2, 3, 1).reshape(batch_size, 196, 16, 16, 3)\n",
    "    y = recon_model(final_embs).reshape(batch_size, 196, 16, 16, 3)\n",
    "    highs = (norms==True)\n",
    "    lows = (norms==False)\n",
    "\n",
    "    high_norm_patches = y[highs]\n",
    "    high_norm_patches_label = sample_images[highs]\n",
    "    low_norm_patches = y[lows]\n",
    "    low_norm_patches_label = sample_images[lows]\n",
    "\n",
    "    high_norm_patches_error += F.mse_loss(high_norm_patches, high_norm_patches_label).detach().numpy()\n",
    "    low_norm_patches_error += F.mse_loss(low_norm_patches, low_norm_patches_label).detach().numpy()\n",
    "\n",
    "\n",
    "print(\"High norm error:\", high_norm_patches_error)\n",
    "print(\"Low norm error:\", low_norm_patches_error)\n",
    "print(\"Ratio:\", high_norm_patches_error/low_norm_patches_error*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
