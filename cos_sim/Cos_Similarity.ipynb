{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3zPNHlEtP96r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Evironment setup and imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import timm\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uIz09qMtP98x"
   },
   "outputs": [],
   "source": [
    "# DeiT-III Classes\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None,\n",
    "                 attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(\n",
    "            B, N, 3, self.num_heads, C // self.num_heads\n",
    "        ).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample.\"\"\"\n",
    "    def __init__(self, drop_prob=0.):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0. or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,)*(x.ndim-1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    # trivial init\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean, std)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # shape [B, embed_dim, H_patch, W_patch]\n",
    "        B, C, H, W = x.shape\n",
    "        # flatten: [B, C, H*W] => transpose => [B, H*W, C]\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x  # [B, N, C]\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Basic Transformer Block (no layer-scale)\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                              qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class Layer_scale_init_Block(nn.Module):\n",
    "    \"\"\"Block w/ layer-scale initialization\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm, init_values=1e-4):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias,\n",
    "                              qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n",
    "                       act_layer=act_layer, drop=drop)\n",
    "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class vit_models(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with optional layer-scale blocks\n",
    "    (based on your code).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_chans=3,\n",
    "        num_classes=1000,\n",
    "        embed_dim=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=False,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.,\n",
    "        attn_drop_rate=0.,\n",
    "        drop_path_rate=0.,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        global_pool=None,\n",
    "        block_layers=Block,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
    "\n",
    "        # DropPath rate\n",
    "        dpr = [drop_path_rate for i in range(depth)]\n",
    "        blocks = []\n",
    "        for i in range(depth):\n",
    "            blk = block_layers(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "            blocks.append(blk)\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        # shape => [B, N, D]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        # add pos embed\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed[:, : x.shape[1], :]\n",
    "\n",
    "        # pass blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]  # cls token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# A small helper to create \"deit_small_patch16_LS\" as in your code:\n",
    "def deit_small_patch16_LS(pretrained=False, img_size=224, **kwargs):\n",
    "    model = vit_models(\n",
    "        img_size=img_size,\n",
    "        patch_size=16,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4,\n",
    "        qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        block_layers=Layer_scale_init_Block,\n",
    "        **kwargs\n",
    "    )\n",
    "    # If pretrained is True, you'd load a checkpoint here\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE9jXcM7P9_Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 (ViT-B14)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /home/renan/.cache/torch/hub/main.zip\n",
      "/home/renan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/renan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/renan/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "DINOv2 loaded.\n",
      "Loading your custom DeiT-III from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154565/2025946435.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting pos_embed from [1,196,384] -> [1,197,384].\n",
      "pos_embed successfully reshaped.\n",
      "Checkpoint checkpoints/source_checkpoints/checkpoint799.pth loaded!\n"
     ]
    }
   ],
   "source": [
    "# Dinov2 + Retrained DeiT-III model\n",
    "\n",
    "# -- DINOv2 (ViT-B14) from torch.hub\n",
    "print(\"Loading DINOv2 (ViT-g14)...\")\n",
    "dino_model = torch.hub.load(\n",
    "    'facebookresearch/dinov2',\n",
    "    'dinov2_vitg14',\n",
    "    pretrained=True,\n",
    "    force_reload=True\n",
    ")\n",
    "dino_model.eval()\n",
    "dino_model.to(device)\n",
    "print(dino_model.n_blocks)\n",
    "\n",
    "print(\"DINOv2 loaded.\")\n",
    "\n",
    "# -- Your custom DeiT-III\n",
    "print(\"Loading your custom DeiT-III from checkpoint...\")\n",
    "\n",
    "deit_model = deit_small_patch16_LS(pretrained=False, img_size=224)\n",
    "deit_model.to(device)\n",
    "deit_model.eval()\n",
    "\n",
    "checkpoint_path = 'checkpoints/source_checkpoints/checkpoint799.pth'\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if 'model' in checkpoint:\n",
    "        state_dict = checkpoint['model']\n",
    "    elif 'state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['state_dict']\n",
    "    else:\n",
    "        state_dict = checkpoint\n",
    "\n",
    "    pos_embed_key = \"pos_embed\"\n",
    "    if pos_embed_key in state_dict:\n",
    "        old_pos_embed = state_dict[pos_embed_key]\n",
    "        desired_shape = (1, 197, 384)\n",
    "        if old_pos_embed.shape != torch.Size(desired_shape):\n",
    "            if list(old_pos_embed.shape) == [1,196,384] and desired_shape == (1,197,384):\n",
    "                print(\"Adjusting pos_embed from [1,196,384] -> [1,197,384].\")\n",
    "                new_pos_embed = torch.zeros((1,197,384))\n",
    "                new_pos_embed[:, 1:, :] = old_pos_embed\n",
    "                state_dict[pos_embed_key] = new_pos_embed\n",
    "                print(\"pos_embed successfully reshaped.\")\n",
    "            else:\n",
    "                print(\"pos_embed shape mismatch not automatically fixable.\")\n",
    "\n",
    "    deit_model.load_state_dict(state_dict, strict=False)\n",
    "    print(f\"Checkpoint {checkpoint_path} loaded!\")\n",
    "else:\n",
    "    print(f\"WARNING: no checkpoint found at {checkpoint_path}. Using random initialization.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QU6erQRdP-Bo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Imagenet dataset.\n"
     ]
    }
   ],
   "source": [
    "# Loading Imagenette\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "# dataset = datasets.Imagenette(\n",
    "#     root=\"data_imagenette\",  # Adjust if needed\n",
    "#     download=True,\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# dataset = ImageFolder(\n",
    "#     root=\"../imagenet/ILSVRC/Data/CLS-LOC/train\",\n",
    "#     transform=transform\n",
    "# )\n",
    "dataset = ImageFolder(\n",
    "    root=\"imagenet/torchvision_ImageFolder/val\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# select 1/10th \n",
    "subset = Subset(dataset, np.random.choice(len(dataset), len(dataset)//10, replace=False))\n",
    "dataset = subset\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=20)\n",
    "\n",
    "print(\"Loaded Imagenet dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a_SWNxyP-HF"
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "# --- DINOv2 final patch embeddings (last layer) ---\n",
    "def get_dino_final_patches(model, imgs):\n",
    "    \"\"\"\n",
    "    DINOv2 (ViT-g/14) final embeddings: call get_intermediate_layers with n=12\n",
    "    (since vitb14 has 12 blocks).\n",
    "    If the returned tensor is 3D ([B, N, D]), return as is.\n",
    "    If it is 4D ([B, D, H, W]), reshape to [B, H*W, D].\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        feats = model.get_intermediate_layers(\n",
    "            imgs,\n",
    "            n=model.n_blocks,\n",
    "            return_class_token=False,\n",
    "            norm=False\n",
    "        )\n",
    "        x = feats[-1]\n",
    "        if len(x.shape) == 3:\n",
    "            return x\n",
    "        elif len(x.shape) == 4:\n",
    "            B, D, H, W = x.shape\n",
    "            x = x.permute(0, 2, 3, 1).contiguous()\n",
    "            x = x.view(B, H * W, D)\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape for DINOv2 final patches: \" + str(x.shape))\n",
    "\n",
    "# --- DINOv2 initial patch embeddings (patch_embed) ---\n",
    "def get_dino_initial_patches(model, imgs):\n",
    "    \"\"\"\n",
    "    Right after patch embedding.\n",
    "    If model.patch_embed returns a 4D tensor ([B, D, H, W]), reshape to [B, H*W, D].\n",
    "    If it returns a 3D tensor ([B, N, D]), return as is.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(imgs)\n",
    "        if len(x.shape) == 4:\n",
    "            B, D, H, W = x.shape\n",
    "            x = x.permute(0, 2, 3, 1).contiguous()\n",
    "            x = x.view(B, H * W, D)\n",
    "            return x\n",
    "        elif len(x.shape) == 3:\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected shape in patch_embed output: \" + str(x.shape))\n",
    "\n",
    "# --- DeiT-III final patch embeddings ---\n",
    "def get_deit_final_patches(model, imgs):\n",
    "    \"\"\"\n",
    "    For DeiT-III, replicate a forward pass to get final patch tokens (excluding the CLS token).\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(imgs)\n",
    "        B, N, D = x.shape\n",
    "\n",
    "        # add cls token + pos embed\n",
    "        cls_token = model.cls_token.expand(B, -1, -1)  # [B, 1, D]\n",
    "        pos_embed = model.pos_embed[:, : (N+1), :]\n",
    "        x = torch.cat([cls_token, x], dim=1)  # => [B, N+1, D]\n",
    "        x = x + pos_embed\n",
    "\n",
    "        for blk in model.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = model.norm(x)  # [B, N+1, D]\n",
    "        x = x[:, 1:, :]  # => [B, N, D]\n",
    "    return x\n",
    "\n",
    "# --- DeiT-III initial patch embeddings ---\n",
    "def get_deit_initial_patches(model, imgs):\n",
    "    \"\"\"\n",
    "    Right after patch_embed => [B, N, D].\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = model.patch_embed(imgs)\n",
    "    return x  # [B, N, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T0SB6cKBP-Ja"
   },
   "outputs": [],
   "source": [
    "# Pass #1: Computing the final patch norm with 98% threshold\n",
    "\n",
    "\n",
    "def gather_global_threshold(\n",
    "    final_extraction_func,\n",
    "    model,\n",
    "    loader,\n",
    "    percentile=0.98\n",
    "):\n",
    "    all_norms = []\n",
    "    for (imgs, _) in tqdm(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        final_embs = final_extraction_func(model, imgs)  # [B, N, D]\n",
    "        norms = torch.norm(final_embs, dim=-1)  # [B, N]\n",
    "        all_norms.append(norms.flatten().cpu())\n",
    "    all_norms = torch.cat(all_norms, dim=0)\n",
    "    threshold = torch.quantile(all_norms, percentile).item()\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShuQvCLvP-MD"
   },
   "outputs": [],
   "source": [
    "# Pass #2: Label patches (artefact vs normal) by final score and measure cos in intital space\n",
    "\n",
    "def compute_cosines_labelled_by_final(\n",
    "    final_extraction_func,\n",
    "    initial_extraction_func,\n",
    "    model,\n",
    "    loader,\n",
    "    threshold,\n",
    "    grid_size\n",
    "):\n",
    "    cos_artifact = []\n",
    "    cos_normal = []\n",
    "    neighbor_offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "\n",
    "    def valid_neighbors(r, c):\n",
    "        for (dr, dc) in neighbor_offsets:\n",
    "            rr, cc = r + dr, c + dc\n",
    "            if 0 <= rr < grid_size and 0 <= cc < grid_size:\n",
    "                yield (rr, cc)\n",
    "\n",
    "    for (imgs, _) in tqdm(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        final_embs = final_extraction_func(model, imgs)    # labeling\n",
    "        initial_embs = initial_extraction_func(model, imgs)  # compute cosine similarity\n",
    "\n",
    "        B, N, D = final_embs.shape\n",
    "        norms = torch.norm(final_embs, dim=-1)  # [B, N]\n",
    "        norms_2d = norms.view(B, grid_size, grid_size)\n",
    "\n",
    "        init_2d = initial_embs.view(B, grid_size, grid_size, D)\n",
    "\n",
    "        # for b in range(B):\n",
    "        #     for r in range(grid_size):\n",
    "        #         for c in range(grid_size):\n",
    "        #             if norms_2d[b, r, c] > threshold:\n",
    "        #                 target = cos_artifact\n",
    "        #             else:\n",
    "        #                 target = cos_normal\n",
    "\n",
    "        #             v = init_2d[b, r, c]\n",
    "        #             for (rr, cc) in valid_neighbors(r, c):\n",
    "        #                 nb = init_2d[b, rr, cc]\n",
    "        #                 sim = F.cosine_similarity(\n",
    "        #                     v.unsqueeze(0),\n",
    "        #                     nb.unsqueeze(0),\n",
    "        #                     dim=1\n",
    "        #                 )\n",
    "        #                 target.append(sim.item())\n",
    "\n",
    "        for dr, dc in neighbor_offsets:\n",
    "            if dr == -1:\n",
    "                # For upward neighbor, consider rows 1:H for main and 0:H-1 for neighbor.\n",
    "                main = init_2d[:, 1:, :, :]       # [B, H-1, W, D]\n",
    "                neighbor = init_2d[:, :-1, :, :]    # [B, H-1, W, D]\n",
    "                mask = norms_2d[:, 1:, :] > threshold  # Label based on main patch norm.\n",
    "            elif dr == 1:\n",
    "                # Downward neighbor.\n",
    "                main = init_2d[:, :-1, :, :]\n",
    "                neighbor = init_2d[:, 1:, :, :]\n",
    "                mask = norms_2d[:, :-1, :] > threshold\n",
    "            elif dc == -1:\n",
    "                # Left neighbor.\n",
    "                main = init_2d[:, :, 1:, :]\n",
    "                neighbor = init_2d[:, :, :-1, :]\n",
    "                mask = norms_2d[:, :, 1:] > threshold\n",
    "            elif dc == 1:\n",
    "                # Right neighbor.\n",
    "                main = init_2d[:, :, :-1, :]\n",
    "                neighbor = init_2d[:, :, 1:, :]\n",
    "                mask = norms_2d[:, :, :-1] > threshold\n",
    "            \n",
    "            # Compute cosine similarity in a vectorized fashion.\n",
    "            similarity = F.cosine_similarity(main, neighbor, dim=-1)  # Shape matches mask.\n",
    "            \n",
    "            # Use the boolean mask to separate artifact vs. normal patches.\n",
    "            artifact_sim = similarity[mask]\n",
    "            normal_sim = similarity[~mask]\n",
    "            cos_artifact.append(artifact_sim.cpu())\n",
    "            cos_normal.append(normal_sim.cpu())\n",
    "\n",
    "    return cos_artifact, cos_normal\n",
    "\n",
    "def plot_density(cos_artifact, cos_normal, title):\n",
    "    \"\"\"\n",
    "    Plot kernel density of cosine similarities.\n",
    "    \"\"\"\n",
    "    cos_artifact = np.concatenate([np.array(x).ravel() for x in cos_artifact])\n",
    "    cos_normal = np.concatenate([np.array(x).ravel() for x in cos_normal])\n",
    "\n",
    "    kde_a = gaussian_kde(cos_artifact) if len(cos_artifact) > 1 else None\n",
    "    kde_n = gaussian_kde(cos_normal) if len(cos_normal) > 1 else None\n",
    "\n",
    "    x_range = np.linspace(0, 1, 400)\n",
    "    plt.figure(figsize=(7/2, 5/2))\n",
    "\n",
    "    if kde_n is not None:\n",
    "        plt.plot(x_range, kde_n(x_range), label=\"Normal patches\", color=\"blue\")\n",
    "    if kde_a is not None:\n",
    "        plt.plot(x_range, kde_a(x_range), label=\"Artifact patches\", color=\"orange\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Cosine Similarity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pA4LNyftP-Ry"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DINOv2 (ViT-B14) Analysis ===\n",
      "=== DeiT-III (Custom) Analysis ===\n",
      "DeiT-III threshold @ 98% = 63.8545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:13<00:00, 22.58it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main run\n",
    "\n",
    "def main_analysis():\n",
    "    # ------------------ DINOv2 (ViT-B14) ------------------ #\n",
    "    print(\"=== DINOv2 (ViT-g14) Analysis ===\")\n",
    "    # Pass #1: final norm threshold\n",
    "    dino_threshold = gather_global_threshold(\n",
    "        final_extraction_func=get_dino_final_patches,\n",
    "        model=dino_model,\n",
    "        loader=data_loader,\n",
    "        percentile=0.98\n",
    "    )\n",
    "    # dino_threshold = 150\n",
    "    print(f\"DINOv2 threshold @ 98% = {dino_threshold:.4f}\")\n",
    "\n",
    "    sample_imgs, _ = next(iter(data_loader))\n",
    "    sample_final = get_dino_final_patches(dino_model, sample_imgs.to(device))\n",
    "    dino_grid_size = int(sample_final.shape[1]**0.5)\n",
    "\n",
    "    # Pass #2: measure cosine similarity in initial space\n",
    "    dino_cos_artifact, dino_cos_normal = compute_cosines_labelled_by_final(\n",
    "        final_extraction_func=get_dino_final_patches,\n",
    "        initial_extraction_func=get_dino_initial_patches,\n",
    "        model=dino_model,\n",
    "        loader=data_loader,\n",
    "        threshold=dino_threshold,\n",
    "        grid_size=dino_grid_size\n",
    "    )\n",
    "\n",
    "    plot_density(dino_cos_artifact, dino_cos_normal,\n",
    "                 title=\"DINOv2 (ViT-g14): Cos Similarity\")\n",
    "\n",
    "    # ------------------ DeiT-III (Custom) ------------------ #\n",
    "    print(\"=== DeiT-III (Custom) Analysis ===\")\n",
    "    # Pass #1: final norm threshold\n",
    "    deit_threshold = gather_global_threshold(\n",
    "        final_extraction_func=get_deit_final_patches,\n",
    "        model=deit_model,\n",
    "        loader=data_loader,\n",
    "        percentile=0.98\n",
    "    )\n",
    "\n",
    "    print(f\"DeiT-III threshold @ 98% = {deit_threshold:.4f}\")\n",
    "\n",
    "    sample_final_deit = get_deit_final_patches(deit_model, sample_imgs.to(device))\n",
    "    deit_grid_size = int(sample_final_deit.shape[1]**0.5)\n",
    "\n",
    "    # Pass #2: measure cosine similarity in initial space\n",
    "    deit_cos_artifact, deit_cos_normal = compute_cosines_labelled_by_final(\n",
    "        final_extraction_func=get_deit_final_patches,\n",
    "        initial_extraction_func=get_deit_initial_patches,\n",
    "        model=deit_model,\n",
    "        loader=data_loader,\n",
    "        threshold=deit_threshold,\n",
    "        grid_size=deit_grid_size\n",
    "    )\n",
    "    plot_density(deit_cos_artifact, deit_cos_normal,\n",
    "                 title=\"DeiT-III: Cos Similarity\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvmpiUXTeXir"
   },
   "outputs": [],
   "source": [
    "# PASS #1: Compute final patch norm => 99.5% threshold\n",
    "\n",
    "def gather_global_threshold(\n",
    "    final_extraction_func,\n",
    "    model,\n",
    "    loader,\n",
    "    percentile=0.995\n",
    "):\n",
    "    all_norms = []\n",
    "    for (imgs, _) in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        final_embs = final_extraction_func(model, imgs)  # [B, N, D]\n",
    "        norms = torch.norm(final_embs, dim=-1)  # [B, N]\n",
    "        all_norms.append(norms.flatten().cpu())\n",
    "    all_norms = torch.cat(all_norms, dim=0)\n",
    "    threshold = torch.quantile(all_norms, percentile).item()\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUNowbCoeYKw"
   },
   "outputs": [],
   "source": [
    "# Run for DeiT-III with 99.5% thershold\n",
    "\n",
    "def new_analysis():\n",
    "    # ------------------ DeiT-III (Custom) ------------------ #\n",
    "    print(\"=== DeiT-III (Custom) Analysis ===\")\n",
    "    # Pass #1: final norm threshold\n",
    "    deit_threshold = gather_global_threshold(\n",
    "        final_extraction_func=get_deit_final_patches,\n",
    "        model=deit_model,\n",
    "        loader=data_loader,\n",
    "        percentile=0.995\n",
    "    )\n",
    "    print(f\"DeiT-III threshold @ 99.5% = {deit_threshold:.4f}\")\n",
    "\n",
    "    # Determine grid_size from a sample\n",
    "    sample_imgs, _ = next(iter(data_loader))\n",
    "    sample_final_deit = get_deit_final_patches(deit_model, sample_imgs.to(device))\n",
    "    deit_grid_size = int(sample_final_deit.shape[1]**0.5)\n",
    "\n",
    "    # Pass #2: measure cosine similarity in the initial space\n",
    "    deit_cos_artifact, deit_cos_normal = compute_cosines_labelled_by_final(\n",
    "        final_extraction_func=get_deit_final_patches,\n",
    "        initial_extraction_func=get_deit_initial_patches,\n",
    "        model=deit_model,\n",
    "        loader=data_loader,\n",
    "        threshold=deit_threshold,\n",
    "        grid_size=deit_grid_size\n",
    "    )\n",
    "    plot_density(deit_cos_artifact, deit_cos_normal,\n",
    "                 title=\"DeiT-III with 99.5% threshold\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    new_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".deit_env12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
